{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Vinh Phuc Ta Dang ft Dao Cong Tinh\n",
    "#\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import posenet\n",
    "import numpy as np\n",
    "from constants import *\n",
    "# import tensorflow as tf\n",
    "from scipy.spatial.distance import euclidean\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "\n",
    "VID_EXT_VALIDS = ['.mp4', '.mov']\n",
    "scale_factor = 0.5\n",
    "VIDEO_URI = 0\n",
    "GUI_Enable = False\n",
    "oo = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(keypoint_coords):\n",
    "    first_keypoint = keypoint_coords[0]\n",
    "    nose, feature = first_keypoint[0], []\n",
    "    mn, mx = oo, -oo\n",
    "    for i in range(1, len(first_keypoint)):\n",
    "        dist = euclidean(nose, first_keypoint[i])\n",
    "        feature.append(dist)\n",
    "\n",
    "    feature = np.array(feature)\n",
    "    mx = np.max(feature)\n",
    "    mn = np.min(feature)\n",
    "    # norm = np.linalg.norm(feature)\n",
    "    # normalized_feature = feature/norm\n",
    "    normalized_feature = np.array([(val-mn)/(mx-mn) for val in feature])\n",
    "    return normalized_feature\n",
    "\n",
    "def extractFeatures(VIDEO_URI):\n",
    "    sess = K.get_session()\n",
    "    model_cfg, model_outputs = posenet.load_model(101, sess)\n",
    "    output_stride = model_cfg['output_stride']\n",
    "\n",
    "    cap = cv2.VideoCapture(VIDEO_URI)\n",
    "    flip = False\n",
    "    cap.set(3, 257)\n",
    "    cap.set(4, 257)\n",
    "    frame_count = 0\n",
    "    frame_series, frame_seq = [], []\n",
    "\n",
    "    print('Extracting video ' + VIDEO_URI + '...')\n",
    "    while True:\n",
    "        try:\n",
    "            input_image, display_image, output_scale = posenet.read_cap(\n",
    "                cap, flip=flip, scale_factor=scale_factor, output_stride=output_stride)\n",
    "        except:\n",
    "            print(\"End video %s.%d\\n\" % (VIDEO_URI, len(frame_series)))\n",
    "            break\n",
    "\n",
    "        heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = sess.run(\n",
    "            model_outputs,\n",
    "            feed_dict={'image:0': input_image}\n",
    "        )\n",
    "\n",
    "        pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multi.decode_multiple_poses(\n",
    "            heatmaps_result.squeeze(axis=0),\n",
    "            offsets_result.squeeze(axis=0),\n",
    "            displacement_fwd_result.squeeze(axis=0),\n",
    "            displacement_bwd_result.squeeze(axis=0),\n",
    "            output_stride=output_stride,\n",
    "            min_pose_score=0.15)\n",
    "            \n",
    "        keypoint_coords *= output_scale\n",
    "        normalized_feature = normalize(keypoint_coords)\n",
    "        frame_count += 1\n",
    "        frame_seq.append(normalized_feature)\n",
    "        if frame_count % window_size == 0:\n",
    "            frame_series.append(frame_seq)\n",
    "            frame_seq = []\n",
    "\n",
    "        # TODO this isn't particularly fast, use GL for drawing and display someday...\n",
    "        if GUI_Enable:\n",
    "            overlay_image = posenet.draw_skel_and_kp(\n",
    "                display_image, pose_scores, keypoint_scores, keypoint_coords,\n",
    "                min_pose_score=0.15, min_part_score=0.1)\n",
    "\n",
    "            cv2.imshow('posenet', overlay_image)\n",
    "            # if \"running\" in VIDEO_URI:\n",
    "            #     filename = \"running.png\"\n",
    "            # else:\n",
    "            #     filename = \"walking.png\"\n",
    "            # cv2.imwrite(filename, overlay_image)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            \n",
    "    return np.array(frame_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "############# ACTION: running #############\n",
      "\n",
      "Extracting video pose/datasets/running/Tinh_Running.mov...\n",
      "End video pose/datasets/running/Tinh_Running.mov.142\n",
      "\n",
      "Extracting video pose/datasets/running/Tinh_Running_2.mov...\n",
      "End video pose/datasets/running/Tinh_Running_2.mov.92\n",
      "\n",
      "Extracting video pose/datasets/running/Phuc_Running.mp4...\n",
      "End video pose/datasets/running/Phuc_Running.mp4.25\n",
      "\n",
      "Total: 259\n",
      "\n",
      "############# ACTION: walking #############\n",
      "\n",
      "Extracting video pose/datasets/walking/Tinh_Walking.mov...\n",
      "End video pose/datasets/walking/Tinh_Walking.mov.43\n",
      "\n",
      "Extracting video pose/datasets/walking/Tinh_Walking_2.mov...\n",
      "End video pose/datasets/walking/Tinh_Walking_2.mov.187\n",
      "\n",
      "Extracting video pose/datasets/walking/Phuc_Walking.mp4...\n",
      "End video pose/datasets/walking/Phuc_Walking.mp4.47\n",
      "\n",
      "Total: 277\n"
     ]
    }
   ],
   "source": [
    "actions = np.array([])\n",
    "labels = np.array([])\n",
    "datasets_dir = 'pose/datasets'\n",
    "\n",
    "time_start = time.time()\n",
    "for action in os.listdir(datasets_dir):\n",
    "    action_dir = os.path.join(datasets_dir, action)\n",
    "    if '.DS_Store' not in action_dir:\n",
    "        print(\"\\n############# ACTION: %s #############\\n\" % action)\n",
    "        total = 0\n",
    "        for vid in os.listdir(action_dir):\n",
    "            _, ext = os.path.splitext(vid)\n",
    "            if ext in VID_EXT_VALIDS:\n",
    "                video_path = os.path.join(action_dir, vid)\n",
    "                keyPoints = extractFeatures(video_path)\n",
    "                labels = np.append(labels, np.array(\n",
    "                    [action]*keyPoints.shape[0]))\n",
    "                total += keyPoints.shape[0]\n",
    "                if len(actions) == 0:\n",
    "                    actions = keyPoints\n",
    "                else:\n",
    "                    actions = np.vstack((actions, keyPoints))\n",
    "        print(\"Total: %d\" % total)\n",
    "time_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total-time: 143.713229 (s)\nactions.shape = (536, 10, 16)\nlabels.shape = (536,)\n"
     ]
    }
   ],
   "source": [
    "# View result\n",
    "print('Total-time: %.6f (s)' % (time_end - time_start))\n",
    "print('actions.shape =', actions.shape)\n",
    "print('labels.shape =', labels.shape)\n",
    "\n",
    "# Store data numpy array\n",
    "np.save('Norm_MinMax_10_X.npy', actions)\n",
    "np.save('Norm_MinMax_10_y.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}